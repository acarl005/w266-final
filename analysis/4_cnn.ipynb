{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset and Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Rachel': 0, 'Ross': 1, 'Chandler': 2, 'Monica': 3, 'Joey': 4, 'Phoebe': 5}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data\n",
    "\n",
    "major_dialog_data = pd.read_pickle(\"./datasets/major_dialog_data.pkl\")\n",
    "all_dialog_data = pd.read_pickle(\"./datasets/all_dialog_data.pkl\")\n",
    "\n",
    "with open('./datasets/utterances.pkl', 'rb') as f:\n",
    "    utterances = pickle.load(f)\n",
    "    \n",
    "with open('./datasets/stop_words.pkl', 'rb') as f:\n",
    "    stop_words = pickle.load(f)\n",
    "\n",
    "# Pre-process the data similarly to workbook 0\n",
    "\n",
    "labels = major_dialog_data.speaker\n",
    "\n",
    "speaker_value_counts = all_dialog_data.speaker.value_counts()\n",
    "major_speaker_value_counts = speaker_value_counts[speaker_value_counts > 40]\n",
    "x, y = major_speaker_value_counts.index, major_speaker_value_counts.values\n",
    "\n",
    "num_major_characters = 6\n",
    "\n",
    "# a list of the top characters names\n",
    "major_characters = x[:num_major_characters]\n",
    "# a set of those same names\n",
    "major_characters_set = set(major_characters)\n",
    "\n",
    "# finally, dicts to and from class Ids and their respective names\n",
    "labels_to_ids = {}\n",
    "ids_to_labels = {}\n",
    "for i, major_character in enumerate(major_characters):\n",
    "    labels_to_ids[major_character] = i\n",
    "    ids_to_labels[i] = major_character\n",
    "labels_to_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model - CNN\n",
    "[https://arxiv.org/abs/1408.5882](https://arxiv.org/abs/1408.5882)\n",
    "\n",
    "CNNs can learn more complex, non-linear patterns. We effectively do a 1-D convolution on the words embeddings (going back to Word2Vec). This creates a variable-length feature map for each convolutional filter. We can aggregate the feature map into a single feature using a 1-D max-pooling operation. This furnishes a hidden layer of fixed length (equal to the number of filters).\n",
    "\n",
    "CNNs aren't as easily interpretable as logistic regression, but there are methods to glean some insight from them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import models.cnn as cnn\n",
    "import numpy as np\n",
    "import os, sys\n",
    "import re\n",
    "import tensorflow as tf\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = Path().resolve().joinpath(\"..\")\n",
    "\n",
    "embeddings_path = root_path.joinpath(\"embeddings\", \"newscrawl.300d.W.pos.vectors.gz\")\n",
    "embeddings_url = \"https://www.dropbox.com/s/kguufyc2xcdi8yk/lexvec.enwiki%2Bnewscrawl.300d.W.pos.vectors.gz?dl=1\"\n",
    "\n",
    "# downloads the Word2Vec embeddings (only need to run once)\n",
    "if not os.path.isfile(embeddings_path):\n",
    "    print(\"downloading embeddings...\")\n",
    "    urllib.request.urlretrieve(embeddings_url, embeddings_path)\n",
    "\n",
    "# Uncomment below to unzip file\n",
    "# !gunzip {str(embeddings_path)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the vocabulary with the embeddings\n",
    "embeddings = KeyedVectors.load_word2vec_format(root_path.joinpath(\"embeddings\", \"newscrawl.300d.W.pos.vectors\"), binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cut = math.ceil(major_dialog_data.utterance.count() * 0.85)\n",
    "label_ids = np.array([labels_to_ids[label] for label in labels])\n",
    "\n",
    "cnn_tokenized = []\n",
    "for utterance in major_dialog_data.utterance:\n",
    "    words = re.findall(r\"[a-z0-9']+\", utterance.lower())\n",
    "    # remove dangling apostrophes and 's at the end of words\n",
    "    words = [re.sub(r\"'s?$\", \"\", word) for word in words]\n",
    "    words = [re.sub(r\"'\", \"\", word) for word in words]\n",
    "    cnn_tokenized.append(words)\n",
    "\n",
    "train_x_cnn = cnn_tokenized[:train_cut]\n",
    "train_y_cnn = label_ids[:train_cut]\n",
    "test_x_cnn = cnn_tokenized[train_cut:]\n",
    "test_y_cnn = label_ids[train_cut:]\n",
    "\n",
    "train_x_cnn = np.array([\" \".join(words) for words in train_x_cnn])\n",
    "test_x_cnn = np.array([\" \".join(words) for words in test_x_cnn])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "cnn_model = cnn.CNN(embeddings, num_major_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_step = 20\n",
    "num_epochs = 5\n",
    "batch_size = 128\n",
    "\n",
    "train_writer = tf.summary.FileWriter(\"./cnn-board/train\")\n",
    "test_writer = tf.summary.FileWriter(\"./cnn-board/test\")\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.tables_initializer())\n",
    "    train_writer.add_graph(sess.graph)\n",
    "    \n",
    "    global_step = 0\n",
    "    X = cnn_model.X\n",
    "    Y = cnn_model.Y\n",
    "    keep_prob = cnn_model.keep_prob\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"{} epoch number: {}\".format(datetime.now(), epoch + 1))\n",
    "        train_x_cnn, train_y_cnn = shuffle(train_x_cnn, train_y_cnn)\n",
    "        \n",
    "        for step, x_batch, y_batch in batch_iter(train_x_cnn, train_y_cnn, batch_size):\n",
    "            global_step += 1\n",
    "            sess.run(cnn_model.train_op, feed_dict={ X: x_batch, Y: y_batch, keep_prob: 1 })\n",
    "\n",
    "            # every so often, report the progress of our loss and training accuracy\n",
    "            if step % display_step == 0:\n",
    "                summ = sess.run(cnn_model.merged_summary, feed_dict={ X: x_batch, Y: y_batch })\n",
    "                train_writer.add_summary(summ, global_step=global_step)\n",
    "                train_writer.flush()\n",
    "\n",
    "        test_acc, summ = sess.run([cnn_model.accuracy, cnn_model.merged_summary], feed_dict={ X: test_x_cnn, Y: test_y_cnn })\n",
    "        test_writer.add_summary(summ, global_step=global_step)\n",
    "        test_writer.flush()\n",
    "        print(\"test accuracy = {:.4f}\".format(test_acc))\n",
    "    \n",
    "    # save the model to disk so we can load it up later for use by `./eval.py`\n",
    "    saver.save(sess, \"./cnn-ckpt/model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CNN Evaluation\n",
    "\n",
    "We calculate a confusion matrix and some other metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./cnn-ckpt/model.ckpt\")\n",
    "    sess.run(tf.tables_initializer())\n",
    "    test_predictions = sess.run(cnn_model.predictions, feed_dict={ cnn_model.X: test_x_cnn })\n",
    "    \n",
    "print(classification_report(test_y_cnn, test_predictions, target_names=major_characters))\n",
    "\n",
    "conf_matrix_plot(confusion_matrix(test_y_cnn, test_predictions), major_characters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saliency Maps\n",
    "[http://www.aclweb.org/anthology/N16-1082](http://www.aclweb.org/anthology/N16-1082)\n",
    "\n",
    "This is one technique for rationalizing the inner workings of a neural network. The partial derivatives of the logits w.r.t. the inputs are taken. Those gradients with the highest magnitude can be considered the most \"salient\" features, as those feature contributed *most highly* to the resulting prediction.\n",
    "\n",
    "In this cell, we take a novel line of dialog (the user can make one up), and run the prediction. But most importantly, we generate the saliency map to try to explain **why** the prediction was made."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot shows the time dimension of the sequence on the y-axis, and the embedding dimensions on the x-axis (300 for Word2Vec). The color in each cell is a value of the gradient for this particular prediction. The rows with cells furthest from 0 are the most salient, i.e. brightly colored rows means that word is more important for that prediction. Another way to attribute saliency to each word is by taking the norm of the gradient in that row.\n",
    "\n",
    "In these next cells, we loop through each class and take some of the correct predictions of that class from the test set. Then, we sample some of the correct predictions and plot their saliency maps. This will show if the model is using reasonable things to make its predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input_to_classify = \"i love you chandler do you want to go to the restaurant\"\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./cnn-ckpt/model.ckpt\")\n",
    "    sess.run(tf.tables_initializer())\n",
    "\n",
    "    user_input_pred, = sess.run(cnn_model.predictions, feed_dict={ cnn_model.X: [user_input_to_classify] })\n",
    "    grads = tf.gradients(cnn_model.logits[:, user_input_pred], [cnn_model.embedded])\n",
    "    user_input_grads, = sess.run(grads, feed_dict={ cnn_model.X: [user_input_to_classify] })\n",
    "    \n",
    "sn.heatmap(user_input_grads[0], yticklabels=user_input_to_classify.split(\" \"), xticklabels=False, center=0)\n",
    "plt.title(major_characters[user_input_pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utterances_to_plot = []\n",
    "embedding_gradients_to_plot = []\n",
    "original_indices = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./cnn-ckpt/model.ckpt\")\n",
    "    sess.run(tf.tables_initializer())\n",
    "\n",
    "    for i in range(num_major_characters):\n",
    "        correct_predictions_i = np.where(np.logical_and(test_y_cnn == i, test_predictions == i))\n",
    "        grads = tf.gradients(cnn_model.logits[:, i], [cnn_model.embedded])\n",
    "        embedding_gradients, = sess.run(grads, feed_dict={ cnn_model.X: test_x_cnn })\n",
    "        utterances_to_plot.append(test_x_cnn[correct_predictions_i])\n",
    "        embedding_gradients_to_plot.append(embedding_gradients[correct_predictions_i])\n",
    "        original_indices.append(correct_predictions_i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "def show_heatmaps(utterances, gradients, xtitles, limit=9, offset=0):\n",
    "    \"\"\"takes a set of inputs and gradients, subsets them, and plots a heatmap\"\"\"\n",
    "    plt.figure(figsize=(18, math.ceil(limit * 1.5)))\n",
    "    for i, utterance in enumerate(utterances[offset:offset + limit]):\n",
    "        sentence = utterance.split(\" \")\n",
    "        if len(sentence) > 1:\n",
    "            plt.subplot(math.ceil(limit / 3), 3, i + 1)\n",
    "            to_plot = gradients[offset + i][:len(sentence)]\n",
    "            sn.heatmap(to_plot, yticklabels=sentence, xticklabels=False, center=0)\n",
    "            plt.xlabel(xtitles[offset + i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(\"## \" + major_characters[0]))\n",
    "show_heatmaps(utterances_to_plot[0], embedding_gradients_to_plot[0], original_indices[0], limit=9, offset=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(\"## \" + major_characters[1]))\n",
    "show_heatmaps(utterances_to_plot[1], embedding_gradients_to_plot[1], original_indices[1], limit=9, offset=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(\"## \" + major_characters[2]))\n",
    "show_heatmaps(utterances_to_plot[2], embedding_gradients_to_plot[2], original_indices[2], limit=9, offset=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(\"## \" + major_characters[3]))\n",
    "show_heatmaps(utterances_to_plot[3], embedding_gradients_to_plot[3], original_indices[3], limit=9, offset=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(\"## \" + major_characters[4]))\n",
    "show_heatmaps(utterances_to_plot[4], embedding_gradients_to_plot[4], original_indices[4], limit=9, offset=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(\"## \" + major_characters[5]))\n",
    "show_heatmaps(utterances_to_plot[5], embedding_gradients_to_plot[5], original_indices[5], limit=9)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
